{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/69/jm4j4ly515s2t870ynxcw8w40000gn/T/ipykernel_80431/2027559285.py:17: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  timeframe = datetime.utcnow() - timedelta(days=120)\n",
      "/var/folders/69/jm4j4ly515s2t870ynxcw8w40000gn/T/ipykernel_80431/2027559285.py:28: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  \"created_utc\": datetime.utcfromtimestamp(submission.created_utc),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 131 posts to CSV.\n"
     ]
    }
   ],
   "source": [
    "# Script 1: Reddit Scraper\n",
    "# Purpose: Collect recent posts from target subreddits using keyword filters\n",
    "\n",
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize Reddit API client\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"WOQrgaEZpGvQZ3LHBW1ITA\",\n",
    "    client_secret=\"QJvMm5UUX7-WkLsHSNwSiy_fYryQLw\",\n",
    "    user_agent=\"ChicagoResearchBot/0.1 by u/kateleext\"\n",
    ")\n",
    "\n",
    "# Parameters\n",
    "subreddits = [\"chicago\", \"AskChicago\"]\n",
    "timeframe = datetime.utcnow() - timedelta(days=120)\n",
    "keywords = [\"blackhawks\"]\n",
    "# Data collection\n",
    "posts = []\n",
    "for sub in subreddits:\n",
    "    for keyword in keywords:\n",
    "        for submission in reddit.subreddit(sub).search(keyword, sort=\"new\", limit=100):\n",
    "                posts.append({\n",
    "                    \"subreddit\": sub,\n",
    "                    \"title\": submission.title,\n",
    "                    \"text\": submission.selftext,\n",
    "                    \"created_utc\": datetime.utcfromtimestamp(submission.created_utc),\n",
    "                    \"score\": submission.score,\n",
    "                    \"num_comments\": submission.num_comments,\n",
    "                    \"url\": submission.url\n",
    "                })\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(posts)\n",
    "df.to_csv(\"reddit_chicago_hockey_posts.csv\", index=False)\n",
    "\n",
    "print(f\"Saved {len(df)} posts to CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['subreddit', 'title', 'text', 'created_utc', 'score', 'num_comments', 'url']\n"
     ]
    }
   ],
   "source": [
    "# Script 2: GPT-Based Situational Driver Tagging\n",
    "# Purpose: Use OpenAI's structured output feature to classify Reddit posts by situational drivers and sentiment, but ONLY if the post includes a decision to attend a hockey game. Also includes engagement-level, context-type, and event-stage tagging.\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "\n",
    "# Load the Reddit posts\n",
    "df = pd.read_csv(\"reddit_chicago_hockey_posts.csv\")\n",
    "\n",
    "# First, let's look at the dataframe columns to confirm\n",
    "print(\"Available columns:\", df.columns.tolist())\n",
    "\n",
    "# Define the JSON schema for the response\n",
    "drivers_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"include\": {\"type\": \"boolean\"},\n",
    "        \"decision_summary\": {\"type\": \"string\"},\n",
    "        \"factors_summary\": {\"type\": \"string\"},\n",
    "        \"behavioral_insight\": {\"type\": \"string\"},\n",
    "        \"evidence_quote\": {\"type\": \"string\"},\n",
    "        \"drivers\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"string\"}\n",
    "        },\n",
    "        \"sentiment\": {\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\"Positive\", \"Neutral\", \"Negative\"]\n",
    "        },\n",
    "        \"fan_engagement_level\": {\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\"Observer\", \"Casual Fan\", \"Participant\", \"Deep Fan\"]\n",
    "        },\n",
    "        \"hockey_context_type\": {\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\"Professional\", \"Junior\", \"Recreational\", \"Pickup\", \"Watch Party\", \"Other\"]\n",
    "        },\n",
    "        \"event_stage\": {\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\"Considering\", \"Planning\", \"Attending\", \"Reflecting\"]\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"include\", \"decision_summary\", \"factors_summary\", \"behavioral_insight\", \"evidence_quote\", \"drivers\", \"sentiment\", \"fan_engagement_level\", \"hockey_context_type\", \"event_stage\"],\n",
    "    \"additionalProperties\": False\n",
    "}\n",
    "\n",
    "# Define the driver options and behavioral instruction\n",
    "system_message = \"\"\"You are an assistant trained to analyze Reddit posts and identify situational drivers behind a user's decision to attend a hockey game.\n",
    "\n",
    "Assume the post is related to attending a hockey game if:\n",
    "- The user says they are going to a game (past or future)\n",
    "- They mention a game as part of an itinerary\n",
    "- They are asking about game logistics (tickets, travel, parking, etc.)\n",
    "- They are reflecting on attending a game\n",
    "- They describe buying tickets, watching at the stadium, or planning to go\n",
    "\n",
    "Only set \\\"include\\\": false if there is absolutely no indication the user attended or planned to attend a hockey game — such as general fandom, watching on TV, or abstract mentions of the team.\n",
    "\n",
    "If include = true:\n",
    "- Provide a detailed `decision_summary` and `factors_summary`\n",
    "- In `factors_summary`, go beyond surface-level description. Explain *why* the user is motivated in this situation. Infer any emotional, social, or contextual pressures influencing the choice (e.g. hosting visitors, rare opportunity, personal milestone, weather, etc).\n",
    "- Infer a concise `behavioral_insight` describing the moment of decision-making, emotional state, or conversion trigger\n",
    "- Provide a direct `evidence_quote` from the post that supports your classification\n",
    "\n",
    "### Situational Driver Options:\n",
    "1. Social Bonding – Driven by desire to connect with others, peer planning, group activity.\n",
    "2. Novelty/FOMO – Seeking a new experience or avoiding missing out on something different.\n",
    "3. Convenience – Chosen due to location, ease, or timing.\n",
    "4. Cost/Value Sensitivity – Influenced by price, discounts, or perceived affordability.\n",
    "5. Affective State – Motivated by emotions such as boredom, stress relief, or celebration.\n",
    "6. Weather-Driven – Decision influenced by weather conditions.\n",
    "7. Out-of-Character Behavior – Not typical for the individual, explicitly or implicitly noted.\n",
    "8. Peer Influence – Decision made due to persuasion or invitation from others.\n",
    "9. Spontaneity – No prior planning; made on a whim or last-minute.\n",
    "10. External Stimulus – Influenced by an ad, post, event listing, or recommendation.\n",
    "\"\"\"\n",
    "\n",
    "# Function to classify a single post\n",
    "def classify_post(title, text):\n",
    "    try:\n",
    "        full_content = f\"Title: {title}\\n\\nContent: {text}\"\n",
    "\n",
    "        input_messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": full_content}\n",
    "        ]\n",
    "\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-4o-2024-08-06\",\n",
    "            input=input_messages,\n",
    "            text={\n",
    "                \"format\": {\n",
    "                    \"type\": \"json_schema\",\n",
    "                    \"name\": \"reddit_post_analysis\",\n",
    "                    \"schema\": drivers_schema,\n",
    "                    \"strict\": False\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        result = json.loads(response.output_text)\n",
    "        return result if result.get(\"include\") else None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing post: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing all posts...\n",
      "Processing post 0/131...\n",
      "Processing post 1/131...\n",
      "Processing post 2/131...\n",
      "Processing post 3/131...\n",
      "Processing post 4/131...\n",
      "Processing post 5/131...\n",
      "Processing post 6/131...\n",
      "Processing post 7/131...\n",
      "Processing post 8/131...\n",
      "Processing post 9/131...\n",
      "Processing post 10/131...\n",
      "Processing post 11/131...\n",
      "Processing post 12/131...\n",
      "Processing post 13/131...\n",
      "Processing post 14/131...\n",
      "Processing post 15/131...\n",
      "Processing post 16/131...\n",
      "Processing post 17/131...\n",
      "Processing post 18/131...\n",
      "Processing post 19/131...\n",
      "Processing post 20/131...\n",
      "Processing post 21/131...\n",
      "Processing post 22/131...\n",
      "Processing post 23/131...\n",
      "Processing post 24/131...\n",
      "Processing post 25/131...\n",
      "Processing post 26/131...\n",
      "Processing post 27/131...\n",
      "Processing post 28/131...\n",
      "Processing post 29/131...\n",
      "Processing post 30/131...\n",
      "Processing post 31/131...\n",
      "Processing post 32/131...\n",
      "Processing post 33/131...\n",
      "Processing post 34/131...\n",
      "Processing post 35/131...\n",
      "Processing post 36/131...\n",
      "Processing post 37/131...\n",
      "Processing post 38/131...\n",
      "Processing post 39/131...\n",
      "Processing post 40/131...\n",
      "Processing post 41/131...\n",
      "Processing post 42/131...\n",
      "Processing post 43/131...\n",
      "Processing post 44/131...\n",
      "Processing post 45/131...\n",
      "Processing post 46/131...\n",
      "Processing post 47/131...\n",
      "Processing post 48/131...\n",
      "Processing post 49/131...\n",
      "Processing post 50/131...\n",
      "Processing post 51/131...\n",
      "Processing post 52/131...\n",
      "Processing post 53/131...\n",
      "Processing post 54/131...\n",
      "Processing post 55/131...\n",
      "Processing post 56/131...\n",
      "Processing post 57/131...\n",
      "Processing post 58/131...\n",
      "Processing post 59/131...\n",
      "Processing post 60/131...\n",
      "Processing post 61/131...\n",
      "Processing post 62/131...\n",
      "Processing post 63/131...\n",
      "Processing post 64/131...\n",
      "Processing post 65/131...\n",
      "Processing post 66/131...\n",
      "Processing post 67/131...\n",
      "Processing post 68/131...\n",
      "Processing post 69/131...\n",
      "Processing post 70/131...\n",
      "Processing post 71/131...\n",
      "Processing post 72/131...\n",
      "Processing post 73/131...\n",
      "Processing post 74/131...\n",
      "Processing post 75/131...\n",
      "Processing post 76/131...\n",
      "Processing post 77/131...\n",
      "Processing post 78/131...\n",
      "Processing post 79/131...\n",
      "Processing post 80/131...\n",
      "Processing post 81/131...\n",
      "Processing post 82/131...\n",
      "Processing post 83/131...\n",
      "Processing post 84/131...\n",
      "Processing post 85/131...\n",
      "Processing post 86/131...\n",
      "Processing post 87/131...\n",
      "Processing post 88/131...\n",
      "Processing post 89/131...\n",
      "Processing post 90/131...\n",
      "Processing post 91/131...\n",
      "Processing post 92/131...\n",
      "Processing post 93/131...\n",
      "Processing post 94/131...\n",
      "Processing post 95/131...\n",
      "Processing post 96/131...\n",
      "Processing post 97/131...\n",
      "Processing post 98/131...\n",
      "Processing post 99/131...\n",
      "Processing post 100/131...\n",
      "Processing post 101/131...\n",
      "Processing post 102/131...\n",
      "Processing post 103/131...\n",
      "Processing post 104/131...\n",
      "Processing post 105/131...\n",
      "Processing post 106/131...\n",
      "Processing post 107/131...\n",
      "Processing post 108/131...\n",
      "Processing post 109/131...\n",
      "Processing post 110/131...\n",
      "Processing post 111/131...\n",
      "Processing post 112/131...\n",
      "Processing post 113/131...\n",
      "Processing post 114/131...\n",
      "Processing post 115/131...\n",
      "Processing post 116/131...\n",
      "Processing post 117/131...\n",
      "Processing post 118/131...\n",
      "Processing post 119/131...\n",
      "Processing post 120/131...\n",
      "Processing post 121/131...\n",
      "Processing post 122/131...\n",
      "Processing post 123/131...\n",
      "Processing post 124/131...\n",
      "Processing post 125/131...\n",
      "Processing post 126/131...\n",
      "Processing post 127/131...\n",
      "Processing post 128/131...\n",
      "Processing post 129/131...\n",
      "Processing post 130/131...\n",
      "\n",
      "Analysis complete!\n",
      "Processed 27 qualifying posts\n",
      "Results exported to hockey_post_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "# Process all posts and store results\n",
    "all_results = []\n",
    "print(\"Processing all posts...\")\n",
    "for index, row in df.iterrows():\n",
    "    print(f\"Processing post {index}/{len(df)}...\")\n",
    "    result = classify_post(row['title'], row['text'])\n",
    "    if result:\n",
    "        all_results.append(result)\n",
    "\n",
    "# Create DataFrame from results and export to CSV\n",
    "if all_results:\n",
    "    output_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Export to CSV\n",
    "    output_df.to_csv('hockey_post_analysis.csv', index=False)\n",
    "    print(\"\\nAnalysis complete!\")\n",
    "    print(f\"Processed {len(all_results)} qualifying posts\")\n",
    "    print(\"Results exported to hockey_post_analysis.csv\")\n",
    "else:\n",
    "    print(\"No qualifying posts found in the dataset.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
